{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wU2NWInygcO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_accuracy = 30\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train = pd.read_csv(r\"/content/drive/MyDrive/ECG Project Files/normalized_processed_dataset(1_signal).csv\", delimiter=\",\")\n",
        "targets_numpy = train.Label.values\n",
        "features_numpy = train.loc[:,train.columns != \"Label\"].values\n",
        "features_numpy_length = features_numpy.shape[0]\n",
        "print(features_numpy.shape)\n",
        "processed_data = []\n",
        "processed_labels = []\n",
        "\n",
        "# Data conversion from string into numpy array\n",
        "for i in range (features_numpy_length):\n",
        "  try:\n",
        "    temp_array = features_numpy[i][0].split(',')\n",
        "    temp2_array = []\n",
        "    for j in range(len(temp_array)):\n",
        "      temp2_array.append(float(temp_array[j]))\n",
        "    if(len(temp2_array)<401):\n",
        "      processed_data.append(temp2_array)\n",
        "      processed_labels.append(targets_numpy[i])\n",
        "\n",
        "  except ValueError:\n",
        "    print(\"Line # that contains an unexpected character:\", i)\n",
        "    continue"
      ],
      "metadata": {
        "id": "X1Ghxn2oyp7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding data\n",
        "padded_data = np.stack([np.pad(i,( int((400-len(i))/2) , 400-int((400-len(i))/2)-len(i) ),mode='edge') for i in processed_data])\n",
        "features_numpy = np.array(padded_data)\n",
        "targets_numpy = np.array(processed_labels)"
      ],
      "metadata": {
        "id": "TI7wBWoKyx0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size check\n",
        "print(features_numpy.shape)\n",
        "print(targets_numpy.shape)"
      ],
      "metadata": {
        "id": "P1f_uon1y1Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving numbers for samples in each class\n",
        "\n",
        "# save the original training examples\n",
        "old_train_values = features_numpy\n",
        "old_train_labels = targets_numpy\n",
        "\n",
        "# create empty lists\n",
        "temp_values_N = []\n",
        "temp_values_B = []\n",
        "temp_values_S = []\n",
        "temp_values_V = []\n",
        "temp_values_F = []\n",
        "temp_values_Q = []\n",
        "temp_labels_N = []\n",
        "temp_labels_B = []\n",
        "temp_labels_S = []\n",
        "temp_labels_V = []\n",
        "temp_labels_F = []\n",
        "temp_labels_Q = []\n",
        "\n",
        "# categorize into classes\n",
        "for i in range (len(targets_numpy)):\n",
        "    if targets_numpy[i] == 0:\n",
        "        temp_values_N.append(features_numpy[i])\n",
        "        temp_labels_N.append(0)\n",
        "    elif targets_numpy[i] == 1:\n",
        "        temp_values_B.append(features_numpy[i])\n",
        "        temp_labels_B.append(1)\n",
        "    elif targets_numpy[i] == 2:\n",
        "        temp_values_S.append(features_numpy[i])\n",
        "        temp_labels_S.append(2)\n",
        "    elif targets_numpy[i] == 3:\n",
        "        temp_values_V.append(features_numpy[i])\n",
        "        temp_labels_V.append(3)\n",
        "    elif targets_numpy[i] == 4:\n",
        "        temp_values_F.append(features_numpy[i])\n",
        "        temp_labels_F.append(4)\n",
        "    elif targets_numpy[i] == 5:\n",
        "        temp_values_Q.append(features_numpy[i])\n",
        "        temp_labels_Q.append(5)\n",
        "\n",
        "print(\"N size\", len(temp_values_N))\n",
        "print(\"B size\", len(temp_values_B))\n",
        "print(\"S size\", len(temp_values_S))\n",
        "print(\"V size\", len(temp_values_V))\n",
        "print(\"F size\", len(temp_values_F))\n",
        "print(\"Q size\", len(temp_values_Q))"
      ],
      "metadata": {
        "id": "N9Pn9l7dy2RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "\n",
        "old_train_values = np.append(old_train_values, temp_values_B*4, axis=0)\n",
        "old_train_labels = np.append(old_train_labels, temp_labels_B*4, axis=0)\n",
        "print(old_train_values.shape)\n",
        "\n",
        "old_train_values = np.append(old_train_values, temp_values_S*29, axis=0)\n",
        "old_train_labels = np.append(old_train_labels, temp_labels_S*29, axis=0)\n",
        "print(old_train_values.shape)\n",
        "\n",
        "old_train_values = np.append(old_train_values, temp_values_V*10, axis=0)\n",
        "old_train_labels = np.append(old_train_labels, temp_labels_V*10, axis=0)\n",
        "print(old_train_values.shape)\n",
        "\n",
        "old_train_values = np.append(old_train_values, temp_values_F*87, axis=0)\n",
        "old_train_labels = np.append(old_train_labels, temp_labels_F*87, axis=0)\n",
        "print(old_train_values.shape)\n",
        "\n",
        "old_train_values = np.append(old_train_values, temp_values_Q*8, axis=0)\n",
        "old_train_labels = np.append(old_train_labels, temp_labels_Q*8, axis=0)\n",
        "print(old_train_values.shape)\n",
        "\n",
        "# Adding back into the original numpy arrays\n",
        "targets_numpy = old_train_labels\n",
        "features_numpy = old_train_values"
      ],
      "metadata": {
        "id": "d7xh2RlFy7Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and loading dataloaders\n",
        "\n",
        "# Train to validation data ratio is selected as 80% to 20%, respectively\n",
        "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy, targets_numpy, test_size = 0.2, random_state = 42)\n",
        "featuresTrain = torch.from_numpy(features_train)\n",
        "targetsTrain = torch.from_numpy(targets_train)\n",
        "featuresTest = torch.from_numpy(features_test)\n",
        "targetsTest = torch.from_numpy(targets_test)\n",
        "train = TensorDataset(featuresTrain,targetsTrain)\n",
        "test = TensorDataset(featuresTest,targetsTest)\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# Calculating number of epochs from batch size and number of iterations\n",
        "batch_size = 100\n",
        "n_iters = 35000\n",
        "num_epochs = int(n_iters / (len(features_train) / batch_size))\n",
        "print(\"Epoch #:\",num_epochs)"
      ],
      "metadata": {
        "id": "wv6yblnDzJHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "use_cuda = True"
      ],
      "metadata": {
        "id": "XWUnhRxWzl74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Lets make sure the augmented data is in the right type (non nan)\n",
        "print(features_numpy.shape)\n",
        "for element in features_numpy:\n",
        "  for number in element:\n",
        "    if (math.isnan(number)):\n",
        "      print(\"NaN\")"
      ],
      "metadata": {
        "id": "n1bdAKKmN_fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.layer_dim = layer_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        #self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) #lstm\n",
        "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "        c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "        if next(self.parameters()).is_cuda:\n",
        "            h0 = h0.cuda()\n",
        "            c0 = c0.cuda()\n",
        "        out, (hn,cn) = self.lstm(x, (h0,c0))\n",
        "        out = self.relu(out)\n",
        "        out = self.fc_2(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 100\n",
        "layer_dim = 1\n",
        "output_dim = 6\n",
        "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "w8_4CRY4OHHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_dim = 400\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "count = 0\n",
        "predictions_list = []\n",
        "max_accuracy = 0\n",
        "labels_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (signals, labels) in enumerate(train_loader):\n",
        "\n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          signals = signals.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "\n",
        "        train  = Variable(signals.float().view(-1, seq_dim, input_dim))\n",
        "        labels = Variable(labels)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train)\n",
        "        loss = error(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "\n",
        "        if count % 100 == 0:\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    for signals, labels in test_loader:\n",
        "\n",
        "                        #############################################\n",
        "                        #To Enable GPU Usage\n",
        "                        if use_cuda and torch.cuda.is_available():\n",
        "                          signals = signals.cuda()\n",
        "                          labels = labels.cuda()\n",
        "                        #############################################\n",
        "\n",
        "                        signals = Variable(signals.float().view(-1, seq_dim, input_dim))\n",
        "                        outputs = model(signals)\n",
        "                        predicted = torch.max(outputs.data, 1)[1]\n",
        "                        predictions_list.append(predicted.detach().cpu().numpy())\n",
        "                        labels_list.append(labels.detach().cpu().numpy())\n",
        "                        total += float(labels.size[0])\n",
        "                        if (predicted==labels):\n",
        "                          correct += 1\n",
        "                    accuracy = 100 * correct / total\n",
        "                    loss_list.append(loss.data)\n",
        "                    iteration_list.append(count)\n",
        "                    accuracy_list.append(accuracy)\n",
        "                    if count % 200 == 0:\n",
        "                        print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))\n",
        "\n",
        "                    # save the best performing model\n",
        "                    if (accuracy>max_accuracy):\n",
        "                      max_accuracy = accuracy\n",
        "                      torch.save(model, \"rnn_lstm_model#\"+str(count)+\".pt\")"
      ],
      "metadata": {
        "id": "_ln8PpX8OSWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_list = torch.tensor(accuracy_list).detach().cpu().numpy()\n",
        "iteration_list = torch.tensor(iteration_list).detach().cpu().numpy()\n",
        "loss_list = torch.tensor(loss_list).detach().cpu().numpy()\n",
        "\n",
        "stop_iteration = 90\n",
        "\n",
        "# loss visualization\n",
        "plt.plot(iteration_list[:stop_iteration],loss_list[:stop_iteration])\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"LSTM: Loss vs Number of iteration\")\n",
        "plt.show()\n",
        "\n",
        "# accuracy visualization\n",
        "plt.plot(iteration_list[:stop_iteration],accuracy_list[:stop_iteration],color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"LSTM: Accuracy vs Number of iteration\")\n",
        "plt.savefig('graph.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DRetGC93O45H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}